{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping et recherche sémantique**"
      ],
      "metadata": {
        "id": "wiUutSjzjDvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install qdrant-client sentence-transformers\n",
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "import numpy as np\n",
        "\n",
        "# ====== CONFIGURATION =============\n",
        "BASE_PATH = \"cours_mooc\"\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(MODEL_NAME) # Chargement du modèle\n",
        "\n",
        "\n",
        "# ============ DEFINTION DES FONCTIONS ===============\n",
        "def create_folders(base_path, course_name):\n",
        "    course_path = os.path.join(base_path, course_name)\n",
        "    os.makedirs(os.path.join(course_path, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(course_path, \"videos\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(course_path, \"pdfs\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(course_path, \"json\"), exist_ok=True)\n",
        "    return course_path\n",
        "\n",
        "def download_file(url, folder_path):\n",
        "    try:\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        response = requests.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            with open(filepath, \"wb\") as file:\n",
        "                for chunk in response.iter_content(1024):\n",
        "                    file.write(chunk)\n",
        "            print(f\"Téléchargé : {filename}\")\n",
        "        else:\n",
        "            print(f\"Erreur ({response.status_code}) : {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Problème avec {url} : {e}\")\n",
        "\n",
        "def generate_embeddings(data):\n",
        "    texts = [\n",
        "        data.get(\"description\", \"\"),\n",
        "        \" \".join(data.get(\"objectives\", [])),\n",
        "        data.get(\"prerequisites\", \"\"),\n",
        "        data.get(\"evaluation_mode\", \"\")\n",
        "    ]\n",
        "    texts = [t for t in texts if t]\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "    return {text: emb.tolist() for text, emb in zip(texts, embeddings)}\n",
        "\n",
        "\n",
        "# =========== CREATION DU SCRAPER ==================\n",
        "def scraper(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Erreur lors de la récupération de la page.\")\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    data = {\n",
        "        \"title\": None, \"reference\": None, \"categories\": [],\n",
        "        \"description\": None, \"objectives\": [], \"prerequisites\": None,\n",
        "        \"evaluation_mode\": None, \"duration\": None, \"effort\": None,\n",
        "        \"weekly_pace\": None, \"language\": None, \"image_url\": None,\n",
        "        \"sessions\": [], \"plan_cours\": [], \"videos\": [], \"images\": [],\n",
        "        \"texts\": [], \"tables\": []\n",
        "    }\n",
        "\n",
        "    # Titre\n",
        "    title_tag = soup.find('h1', class_='subheader__title')\n",
        "    if title_tag:\n",
        "        data[\"title\"] = title_tag.text.strip()\n",
        "        print(\"Titre :\", data[\"title\"])\n",
        "    else:\n",
        "        print(\"Titre non trouvé.\")\n",
        "\n",
        "    # Référence, catégories\n",
        "    ref = soup.find('div', class_='subheader__code')\n",
        "    data[\"reference\"] = ref.text.strip() if ref else None\n",
        "\n",
        "    categories = soup.find_all('a', class_='category-badge')\n",
        "    data[\"categories\"] = [c.find('span', class_='category-badge__title').text.strip()\n",
        "                          for c in categories] if categories else []\n",
        "\n",
        "    # Caractéristiques\n",
        "    characteristics = soup.find_all('li', class_='characteristics__item')\n",
        "    for item in characteristics:\n",
        "        text = item.text.lower()\n",
        "        value = item.find('span', class_='characteristics__term')\n",
        "        if not value: continue\n",
        "        value = value.text.strip()\n",
        "        if \"durée\" in text:\n",
        "            data[\"duration\"] = value\n",
        "        elif \"effort\" in text:\n",
        "            data[\"effort\"] = value\n",
        "        elif \"rythme\" in text:\n",
        "            data[\"weekly_pace\"] = value\n",
        "\n",
        "    lang = soup.find('span', property='availableLanguage')\n",
        "    data[\"language\"] = lang.text.replace(\"Langues:\", \"\").strip() if lang else None\n",
        "\n",
        "    image = soup.find('img')\n",
        "    data[\"image_url\"] = image['src'] if image else None\n",
        "\n",
        "    desc_div = soup.find('div', property='description')\n",
        "    data[\"description\"] = desc_div.text.strip() if desc_div else None\n",
        "\n",
        "    skills = soup.find('div', class_='course-detail__skills')\n",
        "    if skills:\n",
        "        data[\"objectives\"] = [li.text.strip() for li in skills.find_all('li')]\n",
        "\n",
        "    prereq = soup.find('div', property='coursePrerequisites')\n",
        "    data[\"prerequisites\"] = prereq.text.strip() if prereq else None\n",
        "\n",
        "    evaluation = soup.find('div', property='educationalCredentialAwarded')\n",
        "    data[\"evaluation_mode\"] = evaluation.text.strip() if evaluation else None\n",
        "\n",
        "    session_blocks = soup.find_all('div', class_='course-detail__run-descriptions')\n",
        "    for s in session_blocks:\n",
        "        data[\"sessions\"].append({\n",
        "            \"start_date\": s.find('dt', string='Inscription').find_next('dd').text.strip() if s.find('dt', string='Inscription') else None,\n",
        "            \"end_date\": s.find('dt', string='Cours').find_next('dd').text.strip() if s.find('dt', string='Cours') else None,\n",
        "        })\n",
        "\n",
        "    sequences = soup.find_all('div', class_='nested-item nested-item--accordion nested-item--1', property='hasPart')\n",
        "    for seq in sequences:\n",
        "        seq_title = seq.find('button', class_='nested-item__title').text.strip()\n",
        "        chapters = seq.find_all('div', class_='nested-item nested-item--list nested-item--2')\n",
        "        chapter_titles = [ch.find('div', class_='nested-item__content').text.strip() for ch in chapters]\n",
        "        data[\"plan_cours\"].append({'sequence': seq_title, 'chapitres': chapter_titles})\n",
        "\n",
        "    course_content = soup.find('div', class_='course-detail')\n",
        "    if course_content:\n",
        "        for a in course_content.find_all('a', href=True):\n",
        "            href = a['href']\n",
        "            if any(ext in href for ext in ['.mp4', '.webm', '.avi']):\n",
        "                data[\"videos\"].append(href)\n",
        "            elif any(ext in href for ext in ['.pdf', '.docx', '.pptx']):\n",
        "                data[\"texts\"].append(href)\n",
        "\n",
        "        for img in course_content.find_all('img', src=True):\n",
        "            data[\"images\"].append(img['src'])\n",
        "\n",
        "        for iframe in course_content.find_all('iframe', src=True):\n",
        "            src = iframe['src']\n",
        "            if any(ext in src for ext in ['.mp4', '.webm']):\n",
        "                data[\"videos\"].append(src)\n",
        "            elif any(ext in src for ext in ['.jpg', '.png']):\n",
        "                data[\"images\"].append(src)\n",
        "\n",
        "        for table in course_content.find_all('table'):\n",
        "            rows = []\n",
        "            for row in table.find_all('tr'):\n",
        "                cols = [td.text.strip() for td in row.find_all('td')]\n",
        "                rows.append(cols)\n",
        "            data[\"tables\"].append(rows)\n",
        "\n",
        "    # ================ Generation des Embeddings =============\n",
        "    data[\"embeddings\"] = generate_embeddings(data)\n",
        "\n",
        "    # ================ Sauvegarde JSON dans json/ du cours =============\n",
        "    output_base = data[\"title\"].replace(\" \", \"_\").replace(\"/\", \"_\") if data[\"title\"] else \"cours\"\n",
        "    course_path = create_folders(BASE_PATH, output_base)\n",
        "    json_path = os.path.join(course_path, \"json\", f\"{output_base}.json\")\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\" Données sauvegardées dans {json_path}\")\n",
        "\n",
        "    # ============ Téléchargements ============\n",
        "    for img in data[\"images\"]:\n",
        "        download_file(img, os.path.join(course_path, \"images\"))\n",
        "    for vid in data[\"videos\"]:\n",
        "        download_file(vid, os.path.join(course_path, \"videos\"))\n",
        "    for txt in data[\"texts\"]:\n",
        "        download_file(txt, os.path.join(course_path, \"pdfs\"))\n",
        "\n",
        "    print(\"Téléchargements terminés.\")\n",
        "\n",
        "    # ============= Stockage dans Qdrant =================\n",
        "    from qdrant_client import QdrantClient\n",
        "    from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "\n",
        "    client = QdrantClient(\n",
        "        url= \"",\n",
        "        api_key=\""\n",
        "    )\n",
        "\n",
        "    collection_name = \"mooc_embeddings\"\n",
        "\n",
        "    if collection_name not in [c.name for c in client.get_collections().collections]:\n",
        "        client.recreate_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
        "        )\n",
        "\n",
        "    points = [\n",
        "        PointStruct(id=i, vector=vec, payload={\"text\": txt, \"title\": data[\"title\"], \"url\": url})\n",
        "        for i, (txt, vec) in enumerate(data[\"embeddings\"].items())\n",
        "    ]\n",
        "\n",
        "    client.upsert(collection_name=collection_name, points=points)\n",
        "    print(\"Embeddings envoyés à Qdrant Cloud.\")\n",
        "\n",
        "\n",
        "# =================== RECHERCHE SEMANTIQUE ==========================\n",
        "def rechercher_question(query_text):\n",
        "    from qdrant_client import QdrantClient\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    client = QdrantClient(\n",
        "        url= \"https://3995c672-f536-4530-a3de-d27bead3659f.us-west-2-0.aws.cloud.qdrant.io\",\n",
        "        api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.eQ6fAmpbzDFdPGhzNYWhuGuh7xTwYOjHGAIUGxl88F4\"\n",
        "    )\n",
        "\n",
        "    collection_name = \"mooc_embeddings\"\n",
        "\n",
        "    # Encodage de la requête\n",
        "    query_vec = model.encode(query_text, convert_to_numpy=True).tolist()\n",
        "\n",
        "    results = client.query_points(collection_name=collection_name, query=query_vec, limit=1, with_payload=True)\n",
        "    print(\"\\n--- Résultats de la recherche ---\")\n",
        "    for r in results.points:\n",
        "        print(f\"Score : {r.score:.4f}\")\n",
        "        print(f\"Titre : {r.payload.get('title')}\")\n",
        "        print(f\"Texte : {r.payload.get('text')}\")\n",
        "        print(f\"Lien : {r.payload.get('url')}\")\n",
        "        print(\"----------------------------\")\n",
        "\n",
        "\n",
        "# ================== EXÉCUTION ====================\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.mooc-batiment-durable.fr/fr/formations/mooc-batiment-risques-techniques-naturels-comprendre-et-agir/\"\n",
        "    scraper(url)\n",
        "\n",
        "    # Recherche\n",
        "    question = input(\"\\nEntrez votre question pour la recherche sémantique : \")\n",
        "    if question.strip():\n",
        "        # Exemple : question = \"Comment prévenir les risques techniques dans la construction ?\"\n",
        "        rechercher_question(question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEliwN2ikhnv",
        "outputId": "77a1246e-2df9-4d3a-e60a-877fd7deb0c5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titre : Bâtiment : risques techniques et naturels, comprendre et agir\n",
            " Données sauvegardées dans cours_mooc/Bâtiment_:_risques_techniques_et_naturels,_comprendre_et_agir/json/Bâtiment_:_risques_techniques_et_naturels,_comprendre_et_agir.json\n",
            "Téléchargé : herbert_julien.jpg__200x200_q85_crop_replace_alpha-%23FFFFFF_subsampling-2_upscale.jpg\n",
            "Téléchargé : merlin_virginie.jpg__200x200_q85_crop_replace_alpha-%23FFFFFF_subsampling-2_upscale.jpg\n",
            "Téléchargé : henry_frederic.jpg__200x200_q85_crop_replace_alpha-%23FFFFFF_subsampling-2_upscale.jpg\n",
            "Téléchargé : aqc-logo-quadrichromie.jpg__200x113_q85_replace_alpha-%23FFFFFF_subsampling-2_upscale.jpg\n",
            "Téléchargé : by-nc-ndeu.png__100x50_q85_crop-smart_replace_alpha-%23FFFFFF_subsampling-2.jpg\n",
            "Téléchargé : by-nc-ndeu.png__100x50_q85_crop-smart_replace_alpha-%23FFFFFF_subsampling-2.jpg\n",
            "Téléchargements terminés.\n",
            "Embeddings envoyés à Qdrant Cloud.\n",
            "\n",
            "Entrez votre question pour la recherche sémantique : Comment prévenir les risques techniques dans la construction ?\n",
            "\n",
            "--- Résultats de la recherche ---\n",
            "Score : 0.6044\n",
            "Titre : Bâtiment : risques techniques et naturels, comprendre et agir\n",
            "Texte : Comprendre les enjeux (techniques, climatiques...) et mieux connaître les grands principes dans le domaine de la construction Être alerté sur les principaux risques (techniques, naturels, organisationnels, parfois juridiques, pathologiques futures...) Connaître les moyens de prévention et en particulier avoir les recommandations sur la conception et la mise en oeuvre, découvrir des bonnes pratiques transversales (compétences, organisation, interfaces...), être sensibilisés à l'entretien/maintenance etc.\n",
            "Lien : https://www.mooc-batiment-durable.fr/fr/formations/mooc-batiment-risques-techniques-naturels-comprendre-et-agir/\n",
            "----------------------------\n"
          ]
        }
      ]
    }
  ]
}
